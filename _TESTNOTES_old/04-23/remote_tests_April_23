experiments 4/23/19:

ssh case@fetch1081
Password: Ca5eWe5tErn



1) update teamCaseFetchRemoteExperiments repo  (git pull)

2) source devel/setup.bash and  catkin_make

3) pre-pose the robot near "home" pose.

4) roslaunch teamCaseFetch_launchers bagger.launch

5) roslaunch teamCaseFetch_launchers final_navigation_bringup.launch
    this should cause an initial "wiggle" motion

6) roslaunch teamCaseFetch_launchers test_coordinator_visit_each_station.launch
  (or rosrun coordinator visit_all,  which is less typing)

7) interact with prompts from visit_all and observe results;

8)  tune hard-coded pose values in pub_des_state_path_client_amcl_correction.cpp,
   and re-run the above

--- vision data -------
   once above is working, grab some (quick, e.g. 3-second) bags of head_camera
  (together with tf topics) keep amcl running, so tf includes robot pose w/rt map
   grab this data for each of the pre-defined locations


----arm motion tests----
keep above running

1) roslaunch teamCaseFetch_launchers  bagger_manip.launch

2)  need a roslaunch for:
run static transforms publisher:
`roslaunch fetch_arm_behavior_server fetch_static_transforms.launch`

start the cartesian-move behavior server:
rosrun fetch_arm_behavior_server fetch_cart_move_as

then:
rosrun cartesian_motion_commander fetch_cartesian_interactive_ac

and respond to  prompts from this node

(may want to run brief bags at different key poses, and label them)



