protocol, 5/1/19

Do the following:

(lift torso first)
rosrun test_fetch_arm_ac fetch_torso_lift_preset

roslaunch manipulation_launch manipulation.launch  (will unfold arm to preying-mantis pose)


*go to each table with parts, and repeat this:

--use the interactive cartesian move ac
--place the gripper in a grasp pose (e.g., fingers nearly touching the table); 
   (e.g., same for all instances, but could vary this)

--place a part between the finger tips; 

--record the grasp pose tf:  rosrun tf tf_echo torso_lift_link generic_gripper_frame
  record this pose in a mnemonic file (e.g., large_gear_grasp_pose1, large_gear_grasp_pose2 ...)

--carefully move the arm out of view

--grab ~3sec of video; name it mnemonically (e.g. large_gear1, large_gear2, ...)

save this data

-------overnight, get image-recognition code running in object_finder----------

tomorrow: try to recognize and grab parts




